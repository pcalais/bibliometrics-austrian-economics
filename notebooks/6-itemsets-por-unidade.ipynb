{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "refs_df = pd.read_csv(\n",
    "    \"../data/processed/refs.csv\",\n",
    "    usecols=lambda c: c != \"context\"\n",
    ")\n",
    "\n",
    "refs_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. L√™ o CSV com o mapeamento autor ‚Üí tradi√ß√£o\n",
    "author_school_df = pd.read_csv(\n",
    "    \"../notebooks/map-author-school.csv\"\n",
    ")\n",
    "\n",
    "# Normaliza√ß√£o b√°sica\n",
    "author_school_df[\"author\"] = author_school_df[\"author\"].str.strip()\n",
    "refs_df[\"author\"] = refs_df[\"author\"].str.strip()\n",
    "\n",
    "# 2. Left join via author\n",
    "refs_df = (\n",
    "    refs_df\n",
    "    .merge(\n",
    "        author_school_df,\n",
    "        on=\"author\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Se tradition n√£o foi encontrada, marcar como 'non-classified'\n",
    "refs_df[\"tradition\"] = refs_df[\"tradition\"].fillna(\"non-classified\")\n",
    "\n",
    "refs_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "refs_df[['author','tradition']].value_counts(dropna=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_k_authors(refs_df, k):\n",
    "    \"\"\"\n",
    "    Mant√©m apenas os top-K autores mais frequentes em refs_df.\n",
    "    \n",
    "    Popularidade = n√∫mero de ocorr√™ncias do autor no dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Conta frequ√™ncia por autor\n",
    "    author_counts = (\n",
    "        refs_df[\"author\"]\n",
    "        .value_counts()\n",
    "    )\n",
    "\n",
    "    # 2. Seleciona top-K autores\n",
    "    top_k_authors = set(author_counts.head(k).index)\n",
    "\n",
    "    # 3. Filtra o dataframe\n",
    "    filtered_refs_df = refs_df[\n",
    "        refs_df[\"author\"].isin(top_k_authors)\n",
    "    ].copy()\n",
    "\n",
    "    return filtered_refs_df\n",
    "\n",
    "\n",
    "# Mant√©m apenas os 100 autores mais citados\n",
    "#refs_df = filter_top_k_authors(refs_df, k=200)\n",
    "\n",
    "print(refs_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_df['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mises_refs_df = pd.read_csv(\"../data/processed/mises_refs.csv\")\n",
    "\n",
    "# Seleciona apenas o que interessa do mises_refs_df\n",
    "mises_parts = (\n",
    "    mises_refs_df[\n",
    "        [\"sentence_id\", \"author\", \"human_action_part_number\"]\n",
    "    ]\n",
    "    .dropna(subset=[\"sentence_id\", \"human_action_part_number\"])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Faz LEFT JOIN em refs_df\n",
    "refs_df = refs_df.merge(\n",
    "    mises_parts,\n",
    "    on=[\"sentence_id\", \"author\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_author_node(row):\n",
    "    if (\n",
    "        row[\"author\"] == \"Mises\"\n",
    "        and pd.notna(row[\"human_action_part_number\"])\n",
    "    ):\n",
    "        return f\"Mises_{(row['human_action_part_number'])}\"\n",
    "    return row[\"author\"]\n",
    "\n",
    "\n",
    "refs_df = refs_df.copy()\n",
    "refs_df[\"mises_part\"] = refs_df.apply(build_author_node, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_df[['author', 'sentence_id', 'paragraph_id', 'section_id', 'paper_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_df[['author', 'sentence_id', 'paragraph_id', 'section_id', 'paper_id']] \\\n",
    "    .isna() \\\n",
    "    .mean() \\\n",
    "    .sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def percentage_units_with_mises(refs_df):\n",
    "    granularities = {\n",
    "        \"sentence\": \"sentence_id\",\n",
    "        \"paragraph\": \"paragraph_id\",\n",
    "        \"section\": \"section_id\",\n",
    "        \"paper\": \"paper_id\",\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for level_name, level_col in granularities.items():\n",
    "\n",
    "        # total de unidades naquele n√≠vel\n",
    "        total_units = (\n",
    "            refs_df[level_col]\n",
    "            .dropna()\n",
    "            .nunique()\n",
    "        )\n",
    "\n",
    "        if total_units == 0:\n",
    "            continue\n",
    "\n",
    "        # unidades que cont√™m Mises\n",
    "        mises_units = (\n",
    "            refs_df.loc[refs_df[\"author\"] == \"Mises\", level_col]\n",
    "            .dropna()\n",
    "            .nunique()\n",
    "        )\n",
    "\n",
    "        percent = 100 * mises_units / total_units\n",
    "\n",
    "        results.append({\n",
    "            \"granularity\": level_name,\n",
    "            \"total_units\": total_units,\n",
    "            \"units_with_mises\": mises_units,\n",
    "            \"percent_with_mises\": percent\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "mises_coverage_df = percentage_units_with_mises(refs_df)\n",
    "mises_coverage_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# --------------------------------------------------\n",
    "# papers que cont√™m Mises\n",
    "# --------------------------------------------------\n",
    "papers_with_mises = set(\n",
    "    refs_df.loc[refs_df[\"author\"] == \"Mises\", \"paper_id\"]\n",
    "    .dropna()\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# todos os papers com t√≠tulo\n",
    "# --------------------------------------------------\n",
    "papers_df = (\n",
    "    refs_df[[\"paper_id\", \"title\", \"filename\"]]\n",
    "    .dropna(subset=[\"paper_id\"])\n",
    "    .drop_duplicates(subset=[\"paper_id\"])\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# papers sem Mises\n",
    "# --------------------------------------------------\n",
    "papers_without_mises_df = papers_df[\n",
    "    ~papers_df[\"paper_id\"].isin(papers_with_mises)\n",
    "]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# sorteia um paper\n",
    "# --------------------------------------------------\n",
    "random_row = papers_without_mises_df.sample(n=1, random_state=None)\n",
    "\n",
    "random_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_metrics_by_granularity(\n",
    "    refs_df,\n",
    "    author_a,\n",
    "    author_b,\n",
    "    granularities=(\"sentence_id\", \"paragraph_id\", \"section_id\", \"paper_id\")\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for g in granularities:\n",
    "        df = refs_df[[g, \"author\"]].dropna()\n",
    "\n",
    "        units = (\n",
    "            df\n",
    "            .groupby(g)[\"author\"]\n",
    "            .agg(set)\n",
    "        )\n",
    "\n",
    "        num_units = len(units)\n",
    "\n",
    "        if num_units == 0:\n",
    "            results.append({\n",
    "                \"granularity\": g,\n",
    "                \"num_units\": 0,\n",
    "                \"support_abs\": 0,\n",
    "                \"support\": 0.0,\n",
    "                \"confidence\": 0.0,\n",
    "                \"lift\": 0.0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        has_a = units.apply(lambda s: author_a in s)\n",
    "        has_b = units.apply(lambda s: author_b in s)\n",
    "\n",
    "        count_a = has_a.sum()\n",
    "        count_b = has_b.sum()\n",
    "        count_ab = (has_a & has_b).sum()\n",
    "\n",
    "        if count_a == 0 or count_b == 0:\n",
    "            confidence = 0.0\n",
    "            lift = 0.0\n",
    "            support = 0.0\n",
    "        else:\n",
    "            support = count_ab / num_units\n",
    "            confidence = count_ab / count_a\n",
    "            lift = confidence / (count_b / num_units)\n",
    "\n",
    "        results.append({\n",
    "            \"granularity\": g,\n",
    "            \"num_units\": num_units,\n",
    "            \"support_abs\": int(count_ab),\n",
    "            \"support\": support,\n",
    "            \"confidence\": confidence,\n",
    "            \"lift\": lift\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pair_metrics_by_granularity(\n",
    "    refs_df,\n",
    "    author_a=\"Lange\",\n",
    "    author_b=\"Mises\"\n",
    ")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_target_stats(refs_df, target_author=\"Mises\"):\n",
    "    granularities = [\"sentence_id\", \"paragraph_id\", \"section_id\", \"paper_id\"]\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    for g in granularities:\n",
    "        units = (\n",
    "            refs_df\n",
    "            .dropna(subset=[g, \"author\"])\n",
    "            .groupby(g)[\"author\"]\n",
    "            .apply(set)\n",
    "        )\n",
    "\n",
    "        num_units = len(units)\n",
    "        units_with_target = units.apply(lambda s: target_author in s)\n",
    "        support_target = units_with_target.mean()\n",
    "\n",
    "        stats[g] = {\n",
    "            \"num_units\": num_units,\n",
    "            \"support_target\": support_target,\n",
    "            \"units\": units  # ‚ö†Ô∏è cache estrutural\n",
    "        }\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_metrics_from_cache(\n",
    "    author,\n",
    "    target_author,\n",
    "    target_stats\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    for g, data in target_stats.items():\n",
    "        units = data[\"units\"]\n",
    "\n",
    "        both = units.apply(\n",
    "            lambda s: author in s and target_author in s\n",
    "        )\n",
    "\n",
    "        support_abs = both.sum()\n",
    "        support = support_abs / data[\"num_units\"]\n",
    "\n",
    "        support_author = units.apply(lambda s: author in s).mean()\n",
    "\n",
    "        confidence = (\n",
    "            support / support_author\n",
    "            if support_author > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        lift = (\n",
    "            confidence / data[\"support_target\"]\n",
    "            if data[\"support_target\"] > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"granularity\": g,\n",
    "            \"num_units\": data[\"num_units\"],\n",
    "            \"support_abs\": support_abs,\n",
    "            \"support\": support,\n",
    "            \"confidence\": confidence,\n",
    "            \"lift\": lift,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_authors_df(\n",
    "    refs_df,\n",
    "    top_k_authors,\n",
    "    target_author=\"Mises\"\n",
    "):\n",
    "    target_stats = precompute_target_stats(\n",
    "        refs_df,\n",
    "        target_author\n",
    "    )\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for author in top_k_authors:\n",
    "        df = pair_metrics_from_cache(\n",
    "            author,\n",
    "            target_author,\n",
    "            target_stats\n",
    "        )\n",
    "\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df = df.assign(\n",
    "            author=author,\n",
    "            with_author=target_author\n",
    "        )\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    authors_df = pd.concat(dfs, ignore_index=True)[\n",
    "        [\"author\", \"with_author\", \"granularity\",\n",
    "         \"num_units\", \"support_abs\", \"support\",\n",
    "         \"confidence\", \"lift\"]\n",
    "    ]\n",
    "\n",
    "    # üîπ Mapa √∫nico author ‚Üí tradition (vem do refs_df)\n",
    "    author_tradition = (\n",
    "        refs_df[[\"author\", \"tradition\"]]\n",
    "        .dropna(subset=[\"author\"])\n",
    "        .drop_duplicates(subset=[\"author\"])\n",
    "    )\n",
    "\n",
    "    # üîπ Left join\n",
    "    authors_df = authors_df.merge(\n",
    "        author_tradition,\n",
    "        on=\"author\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return authors_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_top_k_authors(refs_df, k=20, exclude=(\"Mises\",)):\n",
    "    vc = refs_df[\"author\"].value_counts()\n",
    "    vc = vc.drop(labels=exclude, errors=\"ignore\")\n",
    "    return vc.head(k).index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "K = 1000\n",
    "\n",
    "target_author = \"Mises\"\n",
    "\n",
    "top_k_authors = get_top_k_authors(\n",
    "    refs_df,\n",
    "    k=K,\n",
    "    exclude={target_author}\n",
    ")\n",
    "\n",
    "print(\"Top K authors generated.\")\n",
    "\n",
    "authors_df = build_authors_df(\n",
    "    refs_df,\n",
    "    top_k_authors,\n",
    "    target_author=target_author\n",
    ")\n",
    "\n",
    "authors_df.to_csv(\"../data/processed/author_lifts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Configura√ß√£o\n",
    "# --------------------------------------------------\n",
    "\n",
    "GRANULARITY_ORDER = [\n",
    "    \"sentence_id\",\n",
    "    \"paragraph_id\",\n",
    "    \"section_id\",\n",
    "    \"paper_id\"\n",
    "]\n",
    "\n",
    "def lift_to_sign(lift: float) -> str:\n",
    "    if lift > 1:\n",
    "        return \"+\"\n",
    "    elif lift < 1:\n",
    "        return \"-\"\n",
    "    else:\n",
    "        return \"0\"   # opcional\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Fun√ß√£o principal\n",
    "# --------------------------------------------------\n",
    "\n",
    "def build_pattern_table_from_authors_df(authors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A partir de authors_df (uma linha por author √ó with_author √ó granularidade),\n",
    "    computa:\n",
    "      - pattern de sinais de lift ao longo da granularidade\n",
    "      - count ponderado por support_abs\n",
    "\n",
    "    Retorna tabela ordenada por count (desc):\n",
    "      author | with_author | tradition | pattern | count\n",
    "    \"\"\"\n",
    "\n",
    "    df = authors_df.copy()\n",
    "\n",
    "    # Sinal do lift\n",
    "    df[\"sign\"] = df[\"lift\"].apply(lift_to_sign)\n",
    "\n",
    "    # Garantir ordem das granularidades\n",
    "    df[\"granularity\"] = pd.Categorical(\n",
    "        df[\"granularity\"],\n",
    "        categories=GRANULARITY_ORDER,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # Pivot dos sinais\n",
    "    pivot_sign = (\n",
    "        df\n",
    "        .pivot_table(\n",
    "            index=[\"author\", \"with_author\", \"tradition\"],\n",
    "            columns=\"granularity\",\n",
    "            values=\"sign\",\n",
    "            aggfunc=\"first\"\n",
    "        )\n",
    "        .add_suffix(\"_sign\")\n",
    "    )\n",
    "\n",
    "    # Pivot do support_abs\n",
    "    pivot_support = (\n",
    "        df\n",
    "        .pivot_table(\n",
    "            index=[\"author\", \"with_author\", \"tradition\"],\n",
    "            columns=\"granularity\",\n",
    "            values=\"support_abs\",\n",
    "            aggfunc=\"sum\"\n",
    "        )\n",
    "        .add_suffix(\"_support\")\n",
    "    )\n",
    "\n",
    "    # Junta tudo\n",
    "    pivot = (\n",
    "        pivot_sign\n",
    "        .join(pivot_support)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Construir pattern usando as colunas *_sign\n",
    "    sign_cols = [f\"{g}_sign\" for g in GRANULARITY_ORDER]\n",
    "    pivot[\"pattern\"] = pivot[sign_cols].apply(\n",
    "        lambda row: \"\".join(row.values.astype(str)),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Definir count como soma do support_abs\n",
    "    support_cols = [f\"{g}_support\" for g in GRANULARITY_ORDER]\n",
    "    pivot[\"count\"] = pivot[support_cols].sum(axis=1)\n",
    "\n",
    "    # Tabela final\n",
    "    table = (\n",
    "        pivot[[\"author\", \"with_author\", \"tradition\", \"pattern\", \"count\"]]\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Uso\n",
    "# --------------------------------------------------\n",
    "\n",
    "pattern_table = build_pattern_table_from_authors_df(authors_df)\n",
    "pattern_table.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_pattern_table_from_authors_df(authors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gera uma tabela agregada por:\n",
    "    author, with_author, tradition, pattern\n",
    "\n",
    "    Retorna contagem de ocorr√™ncias (count),\n",
    "    ordenada de forma decrescente.\n",
    "    \"\"\"\n",
    "\n",
    "    table = (\n",
    "        authors_df\n",
    "        .groupby(\n",
    "            [\"author\", \"with_author\", \"tradition\", \"pattern\"],\n",
    "            dropna=False\n",
    "        )\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return table\n",
    "\n",
    "pattern_table = build_pattern_table_from_authors_df(authors_df)\n",
    "\n",
    "pattern_table.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "granularity_order = [\n",
    "    \"sentence_id\",\n",
    "    \"paragraph_id\",\n",
    "    \"section_id\",\n",
    "    \"paper_id\"\n",
    "]\n",
    "\n",
    "df = authors_df.copy()\n",
    "\n",
    "# 1. Sinal do lift\n",
    "if \"lift_relation\" not in df.columns:\n",
    "    df[\"lift_relation\"] = df[\"lift\"].apply(lambda x: \"+\" if x >= 1 else \"-\")\n",
    "\n",
    "# 2. Ordem das granularidades\n",
    "df[\"granularity\"] = pd.Categorical(\n",
    "    df[\"granularity\"],\n",
    "    categories=granularity_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# 3. Uma linha por author √ó tradition\n",
    "author_patterns = (\n",
    "    df\n",
    "    .pivot_table(\n",
    "        index=[\"author\", \"tradition\"],\n",
    "        columns=\"granularity\",\n",
    "        values=\"lift_relation\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reindex(columns=granularity_order)\n",
    ")\n",
    "\n",
    "# 4. Apenas patterns completos\n",
    "author_patterns = author_patterns.dropna()\n",
    "\n",
    "# 5. Pattern multigranular\n",
    "author_patterns[\"pattern\"] = author_patterns.apply(\n",
    "    lambda row: \" \".join(row.values),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "author_patterns = author_patterns.reset_index()\n",
    "\n",
    "# üîπ Total de autores por tradition\n",
    "tradition_totals = (\n",
    "    author_patterns\n",
    "    .groupby(\"tradition\")[\"author\"]\n",
    "    .nunique()\n",
    "    .rename(\"total_authors\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 6. Contagem por tradition √ó pattern\n",
    "pattern_by_tradition = (\n",
    "    author_patterns\n",
    "    .groupby([\"tradition\", \"pattern\"])\n",
    "    .size()\n",
    "    .rename(\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7. Junta total e calcula propor√ß√£o correta\n",
    "pattern_by_tradition = pattern_by_tradition.merge(\n",
    "    tradition_totals,\n",
    "    on=\"tradition\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "pattern_by_tradition[\"proportion\"] = (\n",
    "    pattern_by_tradition[\"count\"] /\n",
    "    pattern_by_tradition[\"total_authors\"]\n",
    ")\n",
    "\n",
    "# 8. Ordena√ß√£o para inspe√ß√£o\n",
    "pattern_by_tradition.sort_values(\n",
    "    [\"tradition\", \"proportion\", \"pattern\"],\n",
    "    ascending=[True, False, True]\n",
    ").head(100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
